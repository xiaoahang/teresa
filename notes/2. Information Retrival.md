#Information Retrieval

## Task defination  

Google's IR system: Finding pages that contain the words in the query. Rank by relevance to the query. By clever indexing(and hardware) so fast.

==Text Retrieval==: Find doucument that are relevant to a user query.

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200128213141122.png" alt="image-20200128213141122" style="zoom:50%;" />

Given: A large, static **document** collection
Given: An information need(keyword-based **query**)
Task: Find all and only documents **relevant** to query

Typical IR systems

- Serach a set of abstracts
- Search newspaper articles
- Library search
- Search the Web

Typically, more statistics than language, but the object to rettirve(and process) is language. 通常情况下统计比语言更多，但是要检索和处理的对象是语言

Formulate a query: **Query type**: normally keywords, could be natural language.

Document are represented as **indexing**.

By **retrieval model** can the system find then best-matching document.

How does the system find it ==efficiently==?

Result are represented to users as **unsorted list/ ranked list/ clusters**

Evaluation to tell whether the system is good.

## Document Indexing

### Manual Approaches

Indexing by **humans**(using fixed **vocabularies**)

**Labour** and training intensive

Large vocabularies(several thousand items)

- Dewey Decimal System
- Library of Congress Subject Headings
- ACM - subfields of CS (ACM Computing Systems(1998))
- Mesh - Medical Subject Headings 
  - a very large controlled vocabulary for describing/indexing medical documents, e.g. journal papers and books
  - provides a hierarchy of descriptors (a.k.a. subject headings)

    - assigned to documents to describe their content
  - hierarchy has a number of top-level categories, e.g.:
    - Anatomy [A]
    - Organisms [B]
    - Diseases [C]
    - Chemicals and Drugs [D]
    - Analytical, Diagnostic and Therapeutic Techniques and Equipment [E]
    - Psychiatry and Psychology [F]
    - Biological Sciences [G]
  - And a number of subcategories (more speciﬁc/detailed terms):
  - And a number of subsubcategories (even more speciﬁc/detailed terms):
  - And a number of subsubsubcategories (yet again more speciﬁc/ detailed terms): 哦吼吼吼
- MEDLINE — Medical Literature Analysis and Retrieval System Online
  - international database of literature for medicine and the life sciences
  - includes papers from ≈5600 diﬀerent sources (mostly journals), in various languages
  - database now holds records for ≈26 million papers
- Each MEDLINE article indexed with 10-15 descriptors from MeSH
  - papers accessed by PubMed search engine interface, using MeSH terms (and other terms, e.g. author name, etc)
  - by default, all descriptors below a given one in the hierarchy are also included in search

Advantages of manual indexing:

- ==High precision== searches
- Word well for ==closed collections==(books in a library)

Problems:

- Searchers need to ==know terms== to achieve high precision
- Labellers need to be ==trained== to achieve consistency
  - Not feasible to expect this from all content creators on the web
- Collections are ==dynamic== -> schemes change constantly

### Automatic Approaches

Term manipulation(certain words count as the same term)

Term weighting(certain terms are more important than others)

Index terms must only derive from text

**Automatic indexing:** 

- No predefined set of index terms
- Instead: use **natural language** as indexing language
- Words in the document give information about its content
- Implementation of indices: **inverted files**
- This is what Google's IR system does
  - at least, it's an important **part** of the story

EXAMPLE:

A small collection of documents:

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200128225627333.png" alt="image-20200128225627333" style="zoom:50%;" />

Say we want to search for word **hot**. How do we do it?

A basic inverted file index:

- Records for each term, the **ids** of the documents in which it appears
- Only matters id it does or does not **appear**  - not how many times

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200128225831238.png" alt="image-20200128225831238" style="zoom:50%;" />

A more sophisticated version:

- also record **count of occurrences** within each document
- Help find documents more relevant to query

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200128230001128.png" alt="image-20200128230001128" style="zoom:50%;" />

A more sophisticated version:

- Also record **position** of each term occurrence within documents
- maybe useful for searching for **phrases** in documents

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200128230142421.png" alt="image-20200128230142421" style="zoom:50%;" />



##Automated Retrival Models

Bag-of-Words Approach:

- Standard approach to representing documents (and queries) in IR:
  - Record what words(terms) are present
  - Usually, plus count of term in each document
- Ignores relations between words
  - i.e. of order, proximity, etc
  - e.g. rabbit eating = eating rabbit
- such representations known as bag-of words
  - c.f. mathematical structure "bag"
    -- like a set (i.e. unordered), but records a count for each element

### Boolean Model

#### Boolean search:

- Binary decision: is documents relevant or not?
- Presence of term is necessary and sufficient for match
- Boolean operators are set operations (AND,OR)

#### Approach:

 Construct complex search commands, by

- combining basic search terms (keywords)
- using boolean operators (AND,OR,NOT,BUT,XOR)
- E.g. Monte-Carlo AND (importance OR stratification) BUT gambling
- Boolean query provides a simple logical basis for deciding whether any document should be returned, based on:
  - whether basic terms of query do/do not appear in the document
  - the meaning of the logical operators

Boolean operators have a set-theoretic intrerpretation for efficient retrieval

Overall document collection forms maximal documents set

Let d(E) denote the document set for ecpression E

- E either a basic term or boolean expression

Boolean operators mao to set-theoretic operations:

- AND →7 ∩ (intersection):   d(E~1~ AND E~2~ ) = d(E~1~ ) ∩ d(E~2~ )
- OR →7 ∪ (union):                 d(E~1~ OR E~2~ ) = d(E~1~ ) ∪ d(E~2~ )
- NOT →7 ^c^ (complement):    d(NOT E) = d(E) ^c^
- BUT →7 − (diﬀerence):        d(E~1~ BUT E~2~ ) = d(E~1~ ) − d(E~2~ )

#### Summary:

Document either match or don't match

- expert knowledge needed to create high-precision quires -> ok for expert users
- often used by bibliographic search engines(library)

Not good for the majority of users:

- Most users not familiar with writing Boolean Quires --> not natural
- Most users don't want to wade through 1000s unranked result lists --> unless very specific search in small collections
- This is particularly true of web search --> large set of docs

### Ranked Retrival Methods

Ranked algorithms:

- Frequency of document terms
- not all search terms necessarily present in document
- Incarnations: 
  - The Vector Space Model (SMART, Salton et al, 1971)
  - The probabilistic model (OKAPI, Robertson/Sparcj Jones, 1976)
  - Web Search Engines

##### The Vector Space Model

Documents are also represented as bag of words

Documents are points in high-dimensional vector space

- each term in index is a dimension --> sparse vectors
- Values are frequencies of terms in documents, or variants of frequency

Queries are also represented as vectors (for terms that exist in index)

Approach:

- Select documrnts(s) with highest document-query similarity

- Document-query similarity is a model for relevance(ranking)

- With ranking, the number of returned documents is less relevant --> users start at the top ans stop when satisfied

  <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129000310798.png" alt="image-20200129000310798" style="zoom:50%;" />

Approach: compare vector of query against vector of each document

- to rank documents according to their similarity to the query

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129000443847.png" alt="image-20200129000443847" style="zoom:50%;" />

How to measure similarity between vectors?

- Each document and the query are represented as a vector of n values:

  $ \vec {d^i} = (d_1^i , d_2^i , . . . , d_n^i ), \vec q ~ = (q_1 , q_2 , . . . , q_n )$

- Many metrics of similarity between 2 vectors, e.g.: Euclidean

  $\sqrt {\sum_{k=1}^n (q_k -d_k)^2}$

  <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129000802379.png" alt="image-20200129000802379" style="zoom:50%;" />

  Is it a good idea? using Euclidean?

  - distance is large for vectors of diﬀerent lengths, even if by only one term (e.g. Doc~2~ and Q)

  - means frequency of terms given too much impact

    <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129000929973.png" alt="image-20200129000929973" style="zoom:50%;" />

- Better similarity metric, used in vector-space model: ==cosine== of the angle between two vectors $\vec x$ and $\vec y$

  $cos(\vec x,\vec y)= \frac {\vec x ·\vec y} {|\vec x||\vec y|} = \frac {\sum_{i=1}^n x_i y_i} {\sqrt {\sum_{i=1}^n x_i^2} {\sqrt {\sum_{i=1}^n y_i^2} } }$

- It can be interpreted as the normalized correlation coefficient:

  i.e. it computes how well the x~i~ and y~i~ correlate, and then divides by the length of the vectors, to scale for their magnitude

  - the vector $\vec x$ is normalized by dividing its components by its length:

    $|\vec x| = \sqrt {\sum_{i=1}^n x_i^2}$

  - The cosine value ranges from:

    - 1, for vectors pointing in the same direction, to
    - 0, for orthogonal vectors, to
    - -1, for vectors point in opposite directions

  - Specialising the equation to comparing a query q and document s:

    $ sim(\vec q, \vec d)= cos(\vec q, \vec d)=\frac {\sum_{i=1}^n q_i d_i} {\sqrt{\sum_{i=1}^n q_i^2} \sqrt{\sum_{i=1}^n d_i^2}}$

    i.e. computes how well occurrences of each term i correlate in query and document, then scales for the magnitude of the overall vectors

## Term Manipulation

What counts as a term?

Common to just use the words, but pre-process them for generalisation

- **Tokenization**: split words from punctuation (get rid of punctuation)

  e.g. word-based. → word based

  three issues: → three issues

- **Capitalisation**: normalise all words to lower (or upper) case

  e.g. Cat and cat should be seen as the same term, but should we conﬂate Turkey and turkey?

- **Lemmatisation**: conﬂate diﬀerent inﬂected forms of a word to their basic form (singular, present tense, 1st person):

  e.g. cats, cat → cat;   have, has, had → have;  worried, worries → worry

- **Stemming**: conﬂate morphological variants by chopping their aﬃx:

  <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129002635957.png" alt="image-20200129002635957" style="zoom:50%;" />

- **Normalisation**: heuristics to conﬂate variants due to spelling, hyphenation, spaces, etc.

  e.g. USA and U.S.A. and U S A → USA

  e.g. chequebook and cheque book → cheque book

  e.g. chequebook and cheque book → cheque book

Worf Frequency and Term Usefulness:

The most and least frequent terms are not the most useful for retrieval

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129002800795.png" alt="image-20200129002800795" style="zoom:80%;" />

### Stemming

### Stopwords

Use Stop list removal to exclude “non-content” words

Usually most frequent (and least useful for retrieval)

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129002856208.png" alt="image-20200129002856208" style="zoom:50%;" />

- greatly reduces the size of the inverted index
- but what if we want to search for phrases that include these terms?
  - Kings of Leon
  - Let it be 
  - To be or not to be 
  - Flights to London

### Single vs. Multi-word Terms

To aid recognition of phrases, might allow multi-word terms

- e.g. Sheﬃeld University

e.g. bigram indexing: store each bigram as a term in index

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129111948762.png" alt="image-20200129111948762" style="zoom:50%;" />

- Problem: number of bigrams is v.large c.f. number of words
  - leads to a huge increase in size of the index
- Alternative: identify multi-word phrases during retrieval
  - Positional indexes, storing position terms in documents, can help
    - use to compute if occurrences of search terms in document are adjacent / close / far apart

Positional indexes:

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129112114780.png" alt="image-20200129112114780" style="zoom:50%;" />



### Term Weighting

What do we use for the inverted index?

- Binary weights - 0/1: whether or not term is present in document
  - But documents with multiple occurrences of query keyword may be more relevant
- Frequency of term in document: like the examples we have seen
  - But what if the term is also frequent in collection?
  - Common terms: not very useful for discriminating relevant documents
- Frequency in document vs in collection: weight terms highly if
  - They are frequent in relevant documents . . . but
  - They are infrequent in collection as a whole

| Key concepts:       |         |                                         |
| ------------------- | ------- | --------------------------------------- |
| Document collection | D       | collection(set) of documents            |
| Size of collection  | \|D\|   | Total number of documents in collection |
| Term freq           | tf~w,d~ | Number of times w occurs in document d  |
| Collection freq     | cf~w~   | number of times w occurs in collection  |
| document freq       | df~w~   | number of documents containing w        |

The informativeness of terms:

- Idea that less common terms are more useful to ﬁnding relevant docs:

  - i.e. these terms are more informative

- Is this idea best addressed using document frequency or collection frequency?

  - Consider following counts (from New York Times data, |D| = 10000):

    <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129112749044.png" alt="image-20200129112749044" style="zoom: 33%;" />

  - term insurance semantically focussed, term try very general

    - document frequency reﬂects this diﬀerence
    - collection frequency fails to distinguish them (i.e. very similar counts)

Informativeness is inversely related to (document) frequency

- less common terms are more useful to ﬁnding relevant documents
  more common terms are less useful to ﬁnding relevant documents

- Compute metric such as: $\frac {|D|} {df_w}$

  - Value reduces as $df_w$ gets larger, tending to 1 as $df_w$approaches $|D|$

    e.g. $\frac {10000}{3997} = 2.5(insurance)$ $\frac {10000} {8760} = 1.14(try)$

  - Value very large for small $df_w$ — over-weights such cases

    e.g. $\frac{10000}{350} = 28.6(mischief)$

- To moderate this, take log: ==Inverse document frequency (idf)==

  - $idf_{w,D} = log \frac{|D|}{df_w}$
   e.g. 
   $log\frac {10000}{3997} = 0.398(insurance)$  
  
   $log\frac {10000} {8760} = 0.057(try) $ $log\frac{10000}{350} = 1.456(mischief)$

BUT Not all terms describe a document equally well

Putting it all together: tf.idf

- Terms which are frequent in a document are better:
  - $tf_{w,d}= freq_{w,d}$
- Terms that are rare in the document collection are better:
  - $idf_{w,D} = log \frac{|D|}{df_w}$
- Combine the two to give tf.idf term weighting:
  - $tf .idf_{w,d,D} = tf_{w,d} · idf_{w,D}$

Most commonly used method for term weighting.

- Used in other ﬁelds too (e.g. summarisation)

TF.IDF EXAMPLE

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129114612974.png" alt="image-20200129114612974" style="zoom:50%;" />

Putting things together:Vector Space Model + tf.idf term weighting + cosine similarity

- tf.idf values for words in two documents D~1~ and D~2~ , and in a query Q “hunter gatherer Scandinavia”:

  <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129125227613.png" alt="image-20200129125227613" style="zoom:50%;" />

  ==$ sim(\vec q, \vec d)= cos(\vec q, \vec d)=\frac {\sum_{i=1}^n q_i d_i} {\sqrt{\sum_{i=1}^n q_i^2} \sqrt{\sum_{i=1}^n d_i^2}}$==

  <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129125325952.png" alt="image-20200129125325952" style="zoom:50%;" />

- so document D 1 is more similar to Q than D 2



## Web Search Ranking

Web docs contain info beyond their mere “textual content”

- state-of-the-art web search engines, like Google, exploit this
- achieve much more eﬀective retrieval than could without it

HTML contains clues that some terms are more important

- e.g. terms in regions marked as title or headings
- e.g. terms emphasised by formatting: bold / bigger / colour
  - can use clever term weighting schemes, that add weight to such terms

Link text — commonly provide description of target doc

- often a better description than doc provides of itself

  e.g. “Hey, here’s a great intro to calculus for beginners – check it out!”

- Google treats link text as part of target doc

- Link structure of web more broadly

  - if page A points to page B, implies B is worth looking at
  - can be used as a measure of authority / quality

### Exploiting Link Structure: the PageRank Algorithm

Key method to exploit link structure of web: PageRank algorithm

- named after its inventor: Larry Page (co-founder of Google)
- assigns a score to each page on web: its PageRank score
  - can be seen to represent the page’s authority (or quality)

PageRank algorithm — key idea:

- link from page A to page B confers authority on B

- how much authority is conferred depends on:

  - the authority (PageRank score) of A, and its number of out-going links

    i.e. A’s authority is shared out amongst its out-going links

  - note that this measure is recursively deﬁned

    score of any page depends on score of every other page

PageRank scores have an alternative interpretation:

- probability that a random surfer will visit that page

  i.e. one who starts at a random page, clicks randomly-chosen links forward, then (getting bored) jumps to a new random page, and so on . . .

  <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129125912841.png" alt="image-20200129125912841" style="zoom:50%;" />

During retrieval, rank score of doc d is a weighted combination of:

- its PageRank score: a measure of its authority
- its IR-Score: how well d matches the query q, based on
  - Vector Space model, TF.IDF, up-weighting of important terms, etc

## Evaluation

There are various retrieval models/algorithms/IR systems, How determine which is the best?

What is the best component/technique for:

- Ranking? (cosine, dot-product, . . . )
- Term selection? (stopword removal, stemming, . . . )
- Term weighting? (binary, TF, TF.IDF, . . . )

How far down the ranked list will a user need to look to ﬁnd some/all relevant items?

Evaluation of eﬀectiveness in relation to the relevance of the documents retrieved

Relevance is judged in a binary way, even if it is in fact a continuous judgement

- Impossible when the task is to rank thousands or millions of options: too subjective, too diﬃcult

Other factors could also be evaluated:

- User eﬀort/ease of use
- Response time
- Form of presentation

In IR research/development scenarios, one cannot aﬀord humans looking at results of every system/variant of system

Instead, performance measured/compared using a pre-created benchmarking corpus, a.k.a. gold-standard dataset, which provides:

- a standard set of documents, and queries
- a list of documents judged relevant for each query, by human subjects
- relevance scores, usually treated as binary

Example: TREC IR evaluation corpora (http://trec.nist.gov/) TREC has run annually since 1991

### Metrics

AIM: 1. get as much good stuﬀ as possible; 2. get as little junk as possible

The two aspects of this aim are addressed by two separate measures — recall and precision

|              | relevant | non-relevant | total   |
| ------------ | -------- | ------------ | ------- |
| Retrieved    | A        | B            | A+B     |
| Not retrived | C        | D            | C+D     |
| Total        | A+C      | B+D          | A+B+C+D |

==recall==: $\frac {A}{A+C}$ = proportion of relevant documents returned

==precision==: $\frac {A}{A+B}$ = proportion of relevant documents retrieved documents that are relevant

Both range [0~1]

Precision and Recall address the relation between the retrieved and relevant sets of documents

Various situations that arise can be pictorially represented in these terms

There is always a trade-oﬀ between precision and recall

For IR: as more results are considered down the list, precision generally drops, while recall generally increases

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129130732942.png" alt="image-20200129130732942" style="zoom:80%;" />

==F measure== (also called F1):combines precision and recall into a single ﬁgure, gives equal weight to both: $F = \frac{2PR}{P+R}$

F is a harmonic mean: penalises low performance in one value more than arithmethic mean:

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129131029786.png" alt="image-20200129131029786" style="zoom:50%;" />

Related measure F β :

- allows user to determine relative importance of P vs. R, by varying β
- F1 is a special case of F~β~ (where β = 1)

### Precision at a cutoﬀ

Measures how well a method ranks relevant documents before non-relevant documents

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129131129710.png" alt="image-20200129131129710" style="zoom:70%;" />

Note precision at top 5 for System 1: inner order of relevant documents doesn’t matter as long as they are all relevant

<img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129131221297.png" alt="image-20200129131221297" style="zoom:80%;" />

**Average Precision**

- Aggregates many precision numbers into one evaluation ﬁgure

- Precision computed for each point a relevant document is found, and ﬁgures averaged

  <img src="/Users/weihangzhang/Library/Application Support/typora-user-images/image-20200129131348420.png" alt="image-20200129131348420" style="zoom:80%;" />





